global:
  slurm:
    image:
      repository: noft133/slurm-headnode-debug
      tag: 25-05-0 # Forcing the developer to set value when deploying new release
      pullPolicy: Always
  database:
    host: pxc-db-proxysql.percona.svc.cluster.local
  kafka:
    host: kafka-slurm-kafka-bootstrap.kafka.svc.cluster.local:9092

slurmctld:
  enabled: true
  replicaCount: 1
  podManagementPolicy: Parallel

  nodeSelector:
    kubernetes.io/hostname: k3d-slurm-cluster-agent-0

  networkSettings:
    hostNetwork: true
    dnsPolicy: ClusterFirstWithHostNet

  podAnnotations:
    fluentbit.io/parser: slurmctld_parser

  securityContext: {}
  resources: {}

  init_node:
    securityContext: {}

  munged:
    securityContext:
      runAsUser: 998
    resources: {}

  storage:
    spool:
      storageClass: local-path
      size: 5Gi

slurmd:
  enabled: false
  replicaCount: 0

  networkSettings: {}

  securityContext:
    privileged: true

  resources:
    requests:
      memory: 2G
      cpu: 2
    limits:
      memory: 2G
      cpu: 2

  init_node:
    securityContext: {}

  munged:
    securityContext:
      runAsUser: 998
    resources: {}

slurmdbd:
  enabled: true
  replicaCount: 1

  nodeSelector:
    kubernetes.io/hostname: k3d-slurm-cluster-agent-0

  networkSettings:
    hostNetwork: true
    dnsPolicy: ClusterFirstWithHostNet

  podAnnotations:
    fluentbit.io/parser: slurmctld_parser

  securityContext: {}
  resources: {}

  init_node:
    securityContext: {}

  munged:
    securityContext:
      runAsUser: 998
    resources: {}

slurmrestd:
  enabled: true
  replicaCount: 3

  hostname: slurmrestd

  podAnnotations:
    fluentbit.io/parser: slurmctld_parser

  securityContext:
    runAsUser: 513
    runAsGroup: 513

  resources: {}

  ingress:
    enabled: true
    annotations: {}
    hosts:
      - host: slurmrestd.apps.slurm.pc8080.pele.local
        paths:
          - path: "/"
            svcPort: 6820
    tls: []

utils:
  imagePullSecrets: []
  affinity: {}
  tolerations: []
  podSecurityContext: {}
  securityContext: {}
  resources: {}
  config:
    mountpoint: /mnt/slurm-config
  jobs:
    - name: slurm-config-distribution
      enabled: true
      annotations:
        argocd.argoproj.io/hook: PostSync
      restartPolicy: "Never"
      backoffLimit: 0
      volumes:
        - name: infra-nfs
          type: nfs
          host: 172.16.200.11
          path: /Slurm
      image:
        repository: artifactory.pele.local/pele-dockers/devops/base/rocky
        tag: "9.1"
        pullPolicy: Always
      script:
        - if ! test -d {{ .Values.utils.config.mountpoint }}/{{ .Release.Name }}; then mkdir {{ .Values.utils.config.mountpoint }}/{{ .Release.Name }}; fi
        - cp /etc/slurm/* {{ .Values.utils.config.mountpoint }}/{{ .Release.Name }}/

slurm_config:
  slurmd_timeout: 300
  ignoreSystemd: false
  
  plugins:
    JobAcctGatherType:
      prefix: jobacct_gather
      types: [cgroup]
    ProctrackType:
      prefix: proctrack
      types: [cgroup]
    TaskPlugin:
      prefix: task
      types: [cgroup,affinity]

  nodes:
    default_properties:
      Boards: 1
      CPUs: 48
      ThreadsPerCore: 1
      RealMemory: 1546336
      Weight: 10
      # Sockets: 2
      # CoresPerSocket: 24
      # MemSpecLimit: 100000
      # CpuSpecList: [0, 24]
      # Gres:
      #   gpu:a100: 0
      #   shard: 0
    ranges:
      - spec:
          prefix: dev-
          parameters:
            - format: "%02d"
              start: 10
              end: 10
            - format: "%02d"
              start: 10
              end: 10
        properties:
          Features: [gpu, cards]

  # gres:
  #   ranges:
  #     - spec:
  #         prefix: dev-
  #         parameters:
  #           - format: "%02d"
  #             start: 10
  #             end: 10
  #           - format: "%02d"
  #             start: 10
  #             end: 10
  #       properties:
  #         AutoDetect: nvml